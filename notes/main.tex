\documentclass{report}
\usepackage{amsthm, amsfonts, amsmath, amssymb}

% Custom Environments
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{oss}{Remark}

%custom commands
\input{./fancy-math-fonts.tex}
\title{Notes}
\author{Fabio Brau}

\begin{document}
\maketitle
\tableofcontents
\chapter{Markov Chains}
In the section we will introduce the model of a classical Markov chain with
discrete time for which each state belongs to some finite or countable set of
possible state. In the next section we extend the definition to states with a
continuous states or in general a continuous density function.
\section{Basic Definitions}
\subsection{Discrete Probability}
Let $\left( \Omega, \FF, \Ph \right)$ a probability space. The class-function $\Ph$
is a finite measure over the sigma-algebra $\FF$ such that
$\Ph\left(\Omega\right)=1$. A discrete random variable is represented by a
measurable function $X:\Omega\to S$, where $S=\left\{ s_1,\dots,s_n,\dots
\right\}$ equipped with the discrete measure. The variable is associated to a discrete
distribution $\lambda$.
\begin{defn}[Discrete Distribution]
A sequence $\lambda=\left( \lambda_0,\dots,\lambda_n,\dots \right)$ is a
\textit{discrete distribution} if and only if $\sum_{i\in\N} \lambda_i = 1$.
\end{defn}

We will say that a random variable $X$ has distribution $\lambda$ if and only
if for each possible outcome $i\in\N$, 
\begin{equation}
  \Ph(X=i):= \Ph\left( \left\{ w\in\Omega\,:\, X(w)=i \right\}
  \right)=\lambda_i.
  \label{eq:discrete-distribution}
\end{equation}

\subsection{Continuous Probability}
As in the discrete case, let $\left( \Omega, \FF, \Ph \right)$ a probability
space. A continuous $d$-dimensional random variable is defined as a measurable map
$X:\Omega\to\R^d$, where $\R^d$ is equipped with the Lebesgue measure.

Remember that by definition, for each $A\subseteq \R^d$ measurable, we have
that the probability that $X$ takes values in the set $A$ corresponds to the
probability measure of the set $X^{-1}(A)$. In formulas, 
\begin{equation}
  \Ph\left(X\in A\right):=\Ph\left( \left\{ w\in\Omega\,:\,X(w)\in A \right\}
  \right)
  \label{eq:prob}
\end{equation}


\begin{defn}[Expectation]
  Let $X$ a random variable. The expectation $\E[X]$, when it exists , is
  defined by
  \begin{equation}
    \E\left[ X \right]:=\int_\Omega X(\omega)\, d\Ph(\omega).
    \label{eq:expectation}
  \end{equation}
\end{defn}

As in the discrete case, is it possible to define the concept of independence
even in the continuous case. In particular we can defined the independence of:
Events, $\sigma$-algebras and random variables.

\begin{defn}[Independence of Events]
  Let $A,B\in\FF$ two events, they are independent if and only if
  \begin{equation}
    \Ph\left( A\cap B \right)=\Ph\left( A \right) \Ph\left(B \right)
  \end{equation}
\end{defn}

\begin{defn}[Independence of $\sigma$-Algebras]
  Two $\sigma$-algebras $\GG_1, \GG_2 \subseteq \FF$ are independent if and
  only if for each two events $A,B$ respectively in $\GG_1, \GG_2$, they are
  independent.
\end{defn}

Before defining the independence of two random variables, let us remember
that a random variable $X$ generate a $\sigma$-algebra on $\Omega$.
\begin{oss}
  Let $X\in\R^d$ be a random variable, the class of set defined by
  \[
    \sigma(X):=\left\{ X^{-1}(A)\,:\, A\subseteq R^d\,\mbox{measurable}
    \right\},
  \]
  is a $\sigma$-algebra.
\end{oss}
\begin{defn}[Independence of Random Variables]
  Two random variables $X,Y$ are independent if and only if the corresponding
  sigma algebras are independent.
\end{defn}

Observe that the last definition is equivalent to say that for each $A,B$
measurable sets, then
\begin{equation}
  \Ph\left( \left( X,Y \right)\in A\times B \right) = \Ph\left( X\in A, Y\in B\right) = \Ph\left(X\in A\right)\Ph\left( Y\in B \right)
\end{equation}

Similar, but not as easily as the discrete case, we can define the concepts
of: Conditional Random Variable (a.k.a \textit{Conditional Expectation}), and
\textit{Conditional probability}.

\begin{prop}[Conditional Expectation]
  Given a random variable $X$ and given a $\sigma$-algebra $\GG\subseteq\FF$,
  there exists a unique random variable $Z$ such that
  \begin{enumerate}
    \item $Z$ is measurable in $\GG$. In formulas $X\in\GG$ or directly
      $\sigma(Z)\subseteq \GG$.
    \item  \(
        \forall G\in\GG,\quad \int_G X(\omega)\,d\Ph(\omega)=\int_G
        Y(\omega)\,d\Ph(\omega).
          \)
  \end{enumerate}
  The random variable $Z$ is named \textit{conditional expectation}, and it is
  usually expressed as $\E\left[ X\,\vert\,\GG \right]:=Z$.
  \begin{proof}
    The proof is a direct consequence of the Theorem of Radon-Nikodym. See
    Durrett for further details.
  \end{proof}
\end{prop}

\begin{defn}[Conditional to a Variable]
  Let $X,Y$ two random variables measurable on $\FF$. The
  \textit{Conditional to a Variable} is defined as follows
  \begin{equation}
    \E\left[ X\,\vert\,Y \right]:=\E\left[ X\,\vert\, \sigma(Y) \right],
  \end{equation}
  where remember that $\sigma(Y)$ is the smallest $\sigma$-algebra for which
  $Y$ is measurable.
\end{defn}

The following two examples aim at providing an idea of the definition of
conditional expectation. The conditional expectation can be thought as the
best estimate of $X$ under the hypothesis that certain events happen.
We focus on two opposite cases, in the first can be
though as the case of \textit{Conditioning by providing all the information}
the latter \textit{Conditioning without adding information}.

\begin{oss}[No-Information]
  Let us assume that $X$ is a random variable measurable on $\GG\subseteq \FF$.
  This means that finding the probability that $X$ takes certain values in
  $B$  by knowing that a set of possible events $\GG$ happen. If the event
  $\left\{ X\in B\right\}$ is in $\GG$, than we are not providing a further
  information. In formulas
  \begin{equation}
    X\in\GG\quad \Rightarrow\quad \E\left[ X\,\vert\,\GG \right] = X.
  \end{equation}
\end{oss}


\begin{oss}[All-Information]
  Let us assume that $X$ is independent of a $\sigma$-algebra $\GG$. Then the
  conditional expectation of $X$ on $\GG$ is given by
  \begin{equation}
    \E[X\,\vert\,\GG]=\E[X].
  \end{equation}
  \begin{proof}
    Let $A\in\GG$. Since $X$ is independent of $\GG$, then, by definition,
    for each $A\in\GG$, $X$ is independent of the characterization function
    over $A$. This implies that
    \begin{equation}
      E\left[ X\,I_A \right] =E\left[ X \right]E\left[ I_A \right]
    \end{equation}
    from which it is easy to deduce the thesis.
  \end{proof}
\end{oss}
\begin{defn}[Density Distribution]
  A random variable $X$ admits a \textit{density distribution} if there
  exists a function $f_X:\R^d\to\R_+$ such that, for any open set
  $A\subseteq\R^d$ the
  following equality holds
  \begin{equation}
    \Ph\left( X\in A \right) := \Ph\left( \left\{ w\in\Omega\,:\,X(w)\in A \right\} \right)= \int_A f_X(x)\,dx
    \label{eq:density}
  \end{equation}
\end{defn}
Observe that not all the random variables admit a density function, a
typical examples is given by the random variable $X\equiv a$, that is equal to the
constant $a$. In this case, for each open set $A\subseteq \R$ we have that
\begin{equation}
  \Ph\left( X\in A \right)=
  \begin{cases}
    1, &\mbox{if } a\in A\\
    0, &\mbox{otherwise}
  \end{cases}
  \label{eq:not_f}
\end{equation}


Given two random variables $X\in\R^m,Y\in\R^n$ we can defined the
joint random variable as follows $(X,Y)\in\R^{m+n}$.
\section{Countable States Markov Chains}
Let $\left( X_t \right)_{t\in\N}$ a sequence of random variables. Let us
assume that each instant $t\in\N$ the variable $X_t$ takes values in a countable state space $S$. 
\begin{defn}[Markov Property]
The sequence $(X_t)_{t\in\N}$ satisfies the Markov property if for each time
$t$ and for each states $s,s_0,\dots,s_n\in S$
\begin{equation}
  \Ph(X_{n+1}=s\,\vert\,
  X_0=s_0,\dots,X_n=s_n)=\Ph(X_{n+1}=s\,\vert\, X_n=s_n).
  \label{eq:markov-property}
\end{equation}
That is, the state assumed at a certain instant $t$ only depends on the
previous state and not on the whole history.
\end{defn}

Observe that, since we are assuming that $S$ is finite, then we are assuming
that there exists an enumeration $S=\left\{ s_1,\dots,s_n,\dots \right\}$.
Hence,  for the sake of simplicity, and without loss of generality, we can
assume that $S=\N$, from which $X_t\in\N$ for each $t$.

Based on the latter assumption over $S$, a \textit{Markov Chain}  with
discrete time and countable state set is represented by a tuple 
$\left( \lambda,P \right)$ as stated in the following definition.

\begin{defn}[Transaction Matrix]
  A transaction matrix $P=\left( p_{ij} \right)$ is matrix with infinite entries 
  such that
  \begin{equation}
    \forall i\in\N,\quad \sum_{j\in\N} p_{ij} = 1.
    \label{eq:transaction-matrix}
  \end{equation}
  In other words, a matrix $P$ is a transaction matrix if every row $P_{i:}$
  is a discrete distribution.
\end{defn}
\begin{defn}[Markov Chain]
  Let $\lambda$ be a discrete distribution, and let $P$ be a transaction
  matrix. A sequence of random variable $\left( x_i \right)_{i\in\N}$ is a
  Markov chain with initial distribution $\lambda$ and transaction matrix
  $P$ 
  \begin{itemize}
    \item $\lambda_i = \Ph\left( X_0=i \right)$;
    \item $p_{ij} = \Ph\left( X_n=j\,\vert\, X_{n-1}=i \right)$ for each
      state $n$.
    \item $X_{n+1}$ is independent from $X_0,\dots,X_{n-1}$.
  \end{itemize}
\end{defn}

\section{Markov Chain with Continuous State Space}
Let assume $\left(X_t \right)_{t\in\N}$ is a sequence of continuous random
variables, and that the initial variable $X_0$ has a continuous
distribution $p(x)$. Remember this means that
\begin{equation}
  \forall A\subseteq \R,\quad \Ph\left( X\in A \right)=\int_A p(x)\,dx
  \label{eq:distribution}
\end{equation}
\begin{defn}[Markov Chain]
  Let $p$ a distribution and let $q(x,y)$ a join distribution. The process
$\left( X_t \right)$ is a Markov chain with continuous state space if
\begin{itemize}
  \item $X_0\sim p$
  \item $\left(X_{t+1}\,\vert\, X_t=x \right) \sim p(x,\cdot)$
  \item $X_{t+1}$ is independent from $X_0,\dots,X_{t-1}$.
\end{itemize}
\end{defn}
\end{document}
