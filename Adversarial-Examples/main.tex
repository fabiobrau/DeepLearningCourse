\documentclass[aspectratio=169, 9pt]{beamer}
\usepackage{bbm}
%\usepackage{algorithm2e}
%\usepackage{mathtools}
%\usepackage{graphicx}
%\usepackage{animate}
%\usepackage{hyperref}
\renewcommand\appendixname{Appendix}
%\usepackage{xcolor}
\usepackage[customcolors]{hf-tikz}
%\tikzset{offset def/.style={
%        above left offset={-0.1,0.35},
%        below right offset={0.1,-0.2},
%    },
%    color def/.style={
%        offset def,
%        set fill color=green,
%        set border color=red,
%    },
%}
\usepackage{marvosym}
\usepackage{fontawesome}
\colorlet{rred}{red!80!black}
\colorlet{ggreen}{green!80!black}
\colorlet{grey}{black!50!white}

\usetheme[progressbar=foot]{metropolis}
\metroset{block=fill}
\usepackage{appendixnumberbeamer}
\setbeamercovered{invisible}

\usepackage{booktabs}
%\usepackage[scale=2]{ccicons}
\usepackage[style=authortitle,backend=bibtex]{biblatex}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\setlength{\abovecaptionskip}{-3pt plus 0pt minus 0pt}

%\usepackage{xspace}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{obs}{Observation}

% math symbols
\input{fancy-math-fonts.tex}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}


\title{Certifiable Robustness only formulas}
\date{March 1, 2023}
\author{Fabio Brau}
\institute{Scuola Superiore Sant'Anna, Pisa.}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\setbeamertemplate{background}{%
    \begin{picture}(300,253)
      \hspace{14.45cm}
       \includegraphics[scale=0.1]{pic/logoretis_noname.png}
   \end{picture}
}
\setbeamercolor{background canvas}{bg=white}
\begin{document}
{\setbeamertemplate{background}{%
    \begin{picture}(300,240)
      \hspace{0.9cm}
       \includegraphics[scale=0.35]{pic/tecip_logo-ENG.png}
       \hspace{0.5cm}
       \includegraphics[scale=0.085]{pic/logoretis.png}
   \end{picture}}%
\maketitle
}
\begin{frame}{Lp norms}

  \[
    \|\delta\|_p = \left( \sum_{i=1}^n |\delta_i|^p\right)^{\frac1p}
  \]
  \[
    \|\delta\|_\infty = \max_{i} |\delta_i|
  \]
  \[
    \|\delta\|_0 = \#\{i\,:\,\delta_i\ne 0\}
  \]
\end{frame}
\begin{frame}
  \[
    \LL :\R^C\times \{1,\cdots,C\}\to \R
  \]
  \[
    \LL(y, c) = -\log\left(\frac{e^{y_c}}{\sum_{i=1}^C e^{y_i}}\right)
  \]
  \[
    \begin{aligned}
      \max_{\delta\in\R^n} \quad & \LL(f(x+\delta), l_{true})\\
      \mbox{s.t.} \quad &\|\delta\|_p\le \varepsilon \\
      & {0\le x+\delta\le 1}\\
      & {Q*(x+\delta)\in\{0,\cdots,Q\}}
    \end{aligned}
  \]
  \[
    \begin{aligned}
      \min_{\delta\in\R^n} \quad & \|\delta\|_p\\
      \mbox{s.t.} \quad & \KK(x+\delta) \ne l_{true} \\
      & {0\le x+\delta\le 1}\\
      & {Q*(x+\delta)\in\{0,\cdots,Q\}}
    \end{aligned}
  \]
\end{frame}
\begin{frame}{Targeted}
  \[
    \begin{aligned}
      \min_{\delta\in\R^n} \quad & \LL(f(x+\delta), l_{target})\\
      \mbox{s.t.} \quad &\|\delta\|_p\le \varepsilon \\
      & {0\le x+\delta\le 1}\\
      & {Q*(x+\delta)\in\{0,\cdots,Q\}}
    \end{aligned}
  \]
  \[
    \begin{aligned}
      \min_{\delta\in\R^n} \quad & \|\delta\|_p\\
      \mbox{s.t.} \quad & \KK(x+\delta) = l_{target} \\
      & {0\le x+\delta\le 1}\\
      & {Q*(x+\delta)\in\{0,\cdots,Q\}}
    \end{aligned}
  \]
  \[
    \delta^* = \varepsilon \,\cdot\, \mbox{sign}\left(\nabla_x \LL(f(x),
    l_{true})\right)
  \]
  \[
    x^* = x+ \delta^*,\quad \|\delta^*\|_\infty=\varepsilon
  \]
  \[
    \tilde \LL(f(x),l) = \alpha\,\cdot\,\LL(f(x),l) 
    \quad+\quad (1-\alpha)\,\cdot\,\LL(f( x+\delta^*), l)
  \]
\end{frame}
\begin{frame}{Szegedy}
  \[
    \begin{aligned}
      \min_{\delta\in\R^n} \quad & c\, \|\delta\|_p +
      \LL(f(x+\delta),l_{target})\\
      \mbox{s.t.} \quad & {0\le x+\delta\le 1}\\
      & {Q*(x+\delta)\in\{0,\cdots,Q\}}
    \end{aligned}
  \]

  \[
    \delta(c)\quad \KK(x+\delta(c)) = l_{target}
  \]
  \[
    c_{left} =0,\, c_{right} = 100,\quad c_{test} = \frac{c_{left} +
    c_{out}}{2}
  \]
  \[
    c_{left} = c_{left}, c_{right} = c_{test}\quad\mbox{if}\quad
    \KK(x+\delta(c_{test})) = l
  \]
  
\end{frame}
\begin{frame}{Carlini}
  \[
    \delta = \frac{1}{2}\left( \tanh(w)+1 \right)-x
  \]
  \[
    \LL(y,l) = \left[ y_l - \max_{j\ne l} y_j \right]^+
  \]
  \[
      \min_{w\in\R^n} \quad & \|\frac{1}{2}\left( \tanh(w)+1 \right)-x\|_p +
      c\,\cdot\, \LL(f(\frac{1}{2}( \tanh(w)+1 )),l_{target})\\
  \]
  \[
      \min_{w\in\R^n} \quad & \|\underbrace{\frac{1}{2}\left( \tanh(w)+1
      \right)-x}_{\color{red}\delta}\|_p +
      c\,\cdot\, \LL(f(\underbrace{\frac{1}{2}( \tanh(w)+1 )}_{\color{red} x+\delta}),l_{target})\\
  \]
\end{frame}
%\begin{frame}{DeepFool}
%  \[
%    f(x) = w^T \, x + b
%  \]
%  \[
%    \delta = - \frac{f(x)}{\|w\|^{2}} w
%  \]
%\end{frame}
\begin{frame}{DeepFool Attack}
\textbf{What is DeepFool Attack?}

DeepFool is a white-box attack that aims to deceive deep neural networks by crafting adversarial examples. The goal of the attack is to perturb an input image in such a way that it is misclassified by the network, while the perturbation is imperceptible to humans.

\vspace{0.5cm}

\textbf{How does it work?}

DeepFool works by iteratively finding the direction of the minimum amount of perturbation required to move an input image across the decision boundary of a neural network. The algorithm computes the gradient of the neural network's output with respect to the input image, and then finds the direction of the smallest perturbation that can cause a change in the predicted class.

\end{frame}

\begin{frame}{DeepFool Attack (cont.)}

\textbf{Iterative algorithm}

The DeepFool algorithm is iterative, and repeats the following steps until the input image is classified as the target class:

Compute the gradient of the neural network's output with respect to the input image.

Calculate the minimum perturbation required to move the input image across the decision boundary of the network.

Add the computed perturbation to the input image.

Check if the image is now classified as the target class. If yes, stop the algorithm. If no, repeat from step 1.

\vspace{0.5cm}

\textbf{Effectiveness}

DeepFool has been shown to be effective in producing imperceptible adversarial examples that fool state-of-the-art neural networks. The attack can be applied to a wide range of deep learning models, including image classifiers and object detectors.

\end{frame}
\begin{frame}{DeepFool Attack (cont.)}

\textbf{Defenses against DeepFool Attack}

Several defenses have been proposed to mitigate the effectiveness of the DeepFool attack, including:

Adversarial training: Training the neural network on a mix of original and adversarial examples.

Defensive distillation: Training the network to output smoothed probabilities instead of hard probabilities.

Input transformation: Applying a non-linear transformation to the input image before feeding it to the neural network.

Gradient masking: Hiding the gradient information from the attacker by adding random noise to the gradients.

\vspace{0.5cm}
\textbf{Conclusion}
The DeepFool attack is a powerful tool for crafting adversarial examples that can deceive deep neural networks. While several defenses have been proposed, the arms race between attacks and defenses continues, highlighting the importance of ongoing research in this area.
\end{frame}
\begin{frame}
  \[
    f(x) = W x + b, \quad
    W = \left[ 
      \begin{array}{c}
        w^{(1)T}\\
        \vdots\\
        w^{(C)T}
      \end{array}
    \right]\in\R^{C\times n}
  \]
  \[
    \delta^{(i)} = - \frac{f_i(x)}{\|w^{(i)}\|^{2}} w^{(i)}
  \]
  \[
    \rho_{adv}(\XX) = \frac{1}{|\XX|}\sum_{x \in\XX}
    \frac{\|\delta(x)\|_2}{\|x\|_2}
  \]
\end{frame}
\begin{frame}{Projected Gd}
  \[
    \begin{aligned}
      \tilde x^{(t)} &= x^{(t-1)} +
    \alpha\,\mbox{sgn}\left( \nabla_x \LL(f(x^{(t-1)}),l_{true}) \right)\\
    x^{(t)} &= \Pi_{B_p(x,\varepsilon)}\left(\tilde x^{(t)}\right)
  \end{aligned}
  \]
  \[
    \min_\theta \rho(\theta),\quad \rho(\theta) = \E_{(x,l)\in\XX} \left[
  \max_{\|\delta\|_p < \varepsilon} \LL\left( f_\theta(x+\delta),l;\theta) \right) \right]
  \]


\end{frame}

\begin{frame}{Distillation}
  \[
    \LL(y, l; T) = -\log\left(\frac{e^{\frac{y_l}{T}}}{\sum_{i=1}^C
    e^{\frac{y_i}{T}}}\right)
  \]
\end{frame}


{%
  \setbeamercolor{background canvas}{bg=black!10!white}
  \setbeamertemplate{background}{%
  \begin{picture}(300,240)
    \hspace{0.9cm}
    \includegraphics[scale=0.35]{pic/tecip_logo-ENG.png}
    \hspace{0.5cm}
    \includegraphics[scale=0.17]{pic/dipe.png}
    \hspace{0.5cm}
    \includegraphics[scale=0.085]{pic/logoretis.png}
  \end{picture}}
\begin{frame}{}
  \textbf{\Huge Thanks for the attention}\\
  \vspace{20pt}
  \begin{minipage}[h]{0.6\textwidth}
  {\large\bf Fabio Brau}
  \vspace{5pt}
  \begin{itemize}
    \item[\faUniversity] {\bf Scuola Superiore Sant'Anna, Pisa}
    \item[\Letter] \texttt{fabio.brau@santannapisa.it}
    \item[\faGlobe] \href{http://retis.santannapisa.it/~f.brau/}{\tt %
                    retis.santannapisa.it/\textasciitilde f.brau}
    \item[\faLinkedin]
      \href{https://www.linkedin.com/in/fabio-brau}{\tt%
        linkedin.com/in/fabio-brau}
  \end{itemize}
  \end{minipage}
\end{frame}
}

\end{document}

